{"cells":[{"cell_type":"markdown","source":["# Chatbot for Schools in Hyderabad\n","\n","## Introduction\n","\n","This notebook contains the implementation of a chatbot designed to assist students and parents in schools located in Hyderabad. The chatbot aims to streamline communication and provide quick access to information.\n","\n","Please follow below instructions to achieve the functionality of the application."],"metadata":{"id":"Dcv5kSB3C3BH"}},{"cell_type":"markdown","source":["## Library Installation\n","\n","To begin with, we need to install the necessary libraries. This includes uninstalling any conflicting packages and installing required dependencies.\n","\n","### Uninstalling Conflicting Packages\n","\n","In this step, we will uninstall `grpcio` to avoid any conflicts."],"metadata":{"id":"SiWgqYiCEgTM"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uWRen4Y0Ru-d","outputId":"90f4a6fe-2bc6-4588-a0f2-75c0f4c2cbe1","executionInfo":{"status":"ok","timestamp":1717031386518,"user_tz":-330,"elapsed":9451,"user":{"displayName":"Suhail Azad","userId":"04231081887048465977"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: grpcio 1.64.0\n","Uninstalling grpcio-1.64.0:\n","  Would remove:\n","    /usr/local/lib/python3.10/dist-packages/grpc/*\n","    /usr/local/lib/python3.10/dist-packages/grpcio-1.64.0.dist-info/*\n","Proceed (Y/n)? y\n","  Successfully uninstalled grpcio-1.64.0\n"]}],"source":["!pip uninstall grpcio"]},{"cell_type":"markdown","source":["##Install Required Packages\n","Install essential packages like `xformers`, `trl`, `peft`, `accelerate`, and `bitsandbytes` without dependencies."],"metadata":{"id":"ht7aq75jEilk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmZ2q64npDaL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716728210971,"user_tz":-330,"elapsed":25479,"user":{"displayName":"Suhail Azad","userId":"04231081887048465977"}},"outputId":"217ec631-1bb3-4079-836a-0dae73022bf7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting xformers<0.0.26\n","  Downloading xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trl\n","  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting peft\n","  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate\n","  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bitsandbytes\n","  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes, xformers, trl, peft, accelerate\n","Successfully installed accelerate-0.30.1 bitsandbytes-0.43.1 peft-0.11.1 trl-0.8.6 xformers-0.0.25.post1\n"]}],"source":["!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"]},{"cell_type":"markdown","source":["##**Installing Milvus**\n"," Install Milvus and its Python client `pymilvus` for managing vector data."],"metadata":{"id":"89vO8XeiFJBp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrv-w1lfQVFb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716728262960,"user_tz":-330,"elapsed":52000,"user":{"displayName":"Suhail Azad","userId":"04231081887048465977"}},"outputId":"59c6e87a-e008-428a-b018-aecf8448ef5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting milvus\n","  Downloading milvus-2.3.5-py3-none-manylinux2014_x86_64.whl (57.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: milvus\n","Successfully installed milvus-2.3.5\n","Collecting pymilvus\n","  Downloading pymilvus-2.4.3-py3-none-any.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools>=67 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (67.7.2)\n","Collecting grpcio<=1.63.0,>=1.49.1 (from pymilvus)\n","  Downloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (3.20.3)\n","Collecting environs<=9.5.0 (from pymilvus)\n","  Downloading environs-9.5.0-py2.py3-none-any.whl (12 kB)\n","Collecting ujson>=2.0.0 (from pymilvus)\n","  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from pymilvus) (2.0.3)\n","Collecting milvus-lite<2.5.0,>=2.4.0 (from pymilvus)\n","  Downloading milvus_lite-2.4.5-py3-none-manylinux2014_x86_64.whl (49.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting marshmallow>=3.0.0 (from environs<=9.5.0->pymilvus)\n","  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv (from environs<=9.5.0->pymilvus)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (2024.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->pymilvus) (1.25.2)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow>=3.0.0->environs<=9.5.0->pymilvus) (24.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\n","Installing collected packages: ujson, python-dotenv, milvus-lite, marshmallow, grpcio, environs, pymilvus\n","Successfully installed environs-9.5.0 grpcio-1.63.0 marshmallow-3.21.2 milvus-lite-2.4.5 pymilvus-2.4.3 python-dotenv-1.0.1 ujson-5.10.0\n"]}],"source":["!pip install milvus\n","!pip install pymilvus"]},{"cell_type":"markdown","source":["##**Installing Gradio**\n","Install `Gradio` for creating a user-friendly interface for the chatbot.\n"],"metadata":{"id":"air4qM3AFzvE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I9Sgcx-t6BIq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716728290385,"user_tz":-330,"elapsed":27434,"user":{"displayName":"Suhail Azad","userId":"04231081887048465977"}},"outputId":"3732eeca-1335-41a1-d576-895427169ff1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-4.31.5-py3-none-any.whl (12.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n","Collecting fastapi (from gradio)\n","  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpy (from gradio)\n","  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.16.4 (from gradio)\n","  Downloading gradio_client-0.16.4-py3-none-any.whl (315 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.9/315.9 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.1)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n","Collecting orjson~=3.0 (from gradio)\n","  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.1)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting python-multipart>=0.0.9 (from gradio)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n","Collecting ruff>=0.2.2 (from gradio)\n","  Downloading ruff-0.4.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Collecting typer<1.0,>=0.12 (from gradio)\n","  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.11.0)\n","Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.4->gradio) (2023.6.0)\n","Collecting websockets<12.0,>=10.0 (from gradio-client==0.16.4->gradio)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n","Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.2)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n","Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n","  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n","  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n","Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (5.10.0)\n","Collecting email_validator>=2.0.0 (from fastapi->gradio)\n","  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n","Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n","  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n","Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n","  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n","  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Building wheels for collected packages: ffmpy\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=eb24ed1386ccbd59609bc2aa562c5348ff7c54621dc2b8367090cd0400185e1e\n","  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n","Successfully built ffmpy\n","Installing collected packages: pydub, ffmpy, websockets, uvloop, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, httptools, h11, dnspython, aiofiles, watchfiles, uvicorn, starlette, httpcore, email_validator, typer, httpx, gradio-client, fastapi-cli, fastapi, gradio\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.9.4\n","    Uninstalling typer-0.9.4:\n","      Successfully uninstalled typer-0.9.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiofiles-23.2.1 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 gradio-4.31.5 gradio-client-0.16.4 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.3 pydub-0.25.1 python-multipart-0.0.9 ruff-0.4.5 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 typer-0.12.3 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-11.0.3\n"]}],"source":["!pip install gradio"]},{"cell_type":"markdown","source":["##Clone Data Repository\n","We will now clone a GitHub repository that contains pre-scraped JSON files. These files contain data scraped using BeautifulSoup from the yellowslate.com website.\n"],"metadata":{"id":"60RVsoAWF5WH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrlgAeqGQw_-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716728291058,"user_tz":-330,"elapsed":676,"user":{"displayName":"Suhail Azad","userId":"04231081887048465977"}},"outputId":"8df132ff-d5f9-4b7a-c5ad-4d78c0db23df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'files_for_chatbot'...\n","remote: Enumerating objects: 18, done.\u001b[K\n","remote: Counting objects: 100% (18/18), done.\u001b[K\n","remote: Compressing objects: 100% (6/6), done.\u001b[K\n","remote: Total 18 (delta 13), reused 14 (delta 12), pack-reused 0\u001b[K\n","Receiving objects: 100% (18/18), 228.60 KiB | 2.18 MiB/s, done.\n","Resolving deltas: 100% (13/13), done.\n"]}],"source":["!git clone https://github.com/Suhail372/files_for_chatbot.git"]},{"cell_type":"markdown","source":["## Import Necessary Libraries\n","After installing the necessary libraries, we import them into our notebook. This includes libraries for data processing, machine learning, and vector database management."],"metadata":{"id":"kSVZVpObGquc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FB-gDyPSQJrk","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1716728296148,"user_tz":-330,"elapsed":5092,"user":{"displayName":"Suhail Azad","userId":"04231081887048465977"}},"outputId":"e749a042-aaa8-4bce-a580-0bd511b0d024"},"outputs":[{"output_type":"error","ename":"ContextualVersionConflict","evalue":"(grpcio 1.64.0 (/usr/local/lib/python3.10/dist-packages), Requirement.parse('grpcio<=1.63.0,>=1.49.1'), {'pymilvus'})","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-92e27cc40bd2>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmilvus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault_server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from pymilvus import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mconnections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mCollectionSchema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFieldSchema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pymilvus/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnSearchRequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRRFRanker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSearchResult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeightedRanker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSearchFuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pymilvus/client/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pymilvus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected string, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \"\"\"\n\u001b[0;32m--> 966\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    825\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m             dist = self._resolve_dist(\n\u001b[0m\u001b[1;32m    828\u001b[0m                 \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace_conflicting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_activate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_resolve_dist\u001b[0;34m(self, req, best, replace_conflicting, env, installer, required_by, to_activate)\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0;31m# Oops, the \"best\" so far conflicts with a dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0mdependent_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mVersionConflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependent_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mContextualVersionConflict\u001b[0m: (grpcio 1.64.0 (/usr/local/lib/python3.10/dist-packages), Requirement.parse('grpcio<=1.63.0,>=1.49.1'), {'pymilvus'})"]}],"source":["import os\n","import json\n","import uuid\n","from transformers import AutoTokenizer, AutoModel\n","from transformers import AutoModelForCausalLM, AutoTokenizer,AutoConfig\n","from milvus import default_server\n","from google.colab import userdata\n","from pymilvus import (\n","    connections, utility, Collection,\n","    CollectionSchema, FieldSchema, DataType\n",")\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import torch\n","import asyncio\n","from transformers import BitsAndBytesConfig\n","import numpy"]},{"cell_type":"markdown","source":["# ChatbotWrapper Class Definition\n","\n","## `__init__(self)`\n","\n","- This is the constructor method of the class.\n","- It initializes various attributes such as the model, tokenizer, device, repository path, and more.\n","- It sets up the environment for the chatbot including the device (CPU or GPU), paths, and collections.\n","\n","## `initialize_model(self)`\n","\n","- Initializes the pre-trained model and tokenizer.\n","- Sets up the model and tokenizer for inference.\n","\n","## `load_json_data(self)`\n","\n","- Loads JSON files containing scraped data from a specified repository path.\n","- Returns a list of paths to the JSON files.\n","\n","## `preprocess(self, json_data)`\n","\n","- Preprocesses each JSON entry into a single text string.\n","- Converts JSON data into a text format suitable for embedding.\n","\n","## `embedding(self, text_data)`\n","\n","- Embeds the preprocessed text data using the pre-trained model.\n","- Computes embeddings for the text data.\n","\n","## `preprocess_and_embed(self, json_files)`\n","\n","- Preprocesses and embeds JSON data from multiple files asynchronously.\n","- Combines preprocessing and embedding steps for efficiency.\n","\n","## `create_collection(self, collection_name)`\n","\n","- Creates a Milvus collection for storing embeddings and associated text data.\n","- Sets up the schema and index for the collection.\n","\n","## `prepare_data(self)`\n","\n","- Prepares data for insertion into the Milvus collection.\n","- Formats embedded data into entities suitable for insertion.\n","\n","## `insert_data(self, collection_name)`\n","\n","- Inserts prepared data into the Milvus collection.\n","- Inserts embedded data along with associated text into the collection.\n","\n","## `load_llama(self)`\n","\n","- Loads a pre-trained llama model and tokenizer.\n","- Sets up the llama model for generating responses.\n","\n","## `run(self, collection_name=None)`\n","\n","- Executes the necessary steps for setting up the chatbot.\n","- Initializes the model, preprocesses and embeds data, inserts data into the collection, and loads the llama model.\n","\n","## `search_milvus(self, query, collection_name)`\n","\n","- Searches the Milvus collection for embeddings similar to the input query.\n","- Retrieves text data associated with similar embeddings.\n","\n","## `generate_chat_response(self, query:str, data:dict) -> str`\n","\n","- Generates a chatbot response based on the input query and search results.\n","- Uses the llama model to generate a concise response.\n","\n","## `search(self, query:str, collection_name=None)`\n","\n","- Handles user queries by searching the Milvus collection and generating responses.\n","- Determines if the input query is a search query or requires context.\n","\n","## `generate_query(self, query:str) -> str`\n","\n","- Generates a query based on previous queries and the current user query.\n","- Uses the llama model to generate a query similar to previous queries.\n","\n","## `is_query(self, query:str) -> bool`\n","\n","- Checks if the input query is a search query.\n","- Identifies keywords indicating a search query.\n","\n","## `not_query_response(self, query:str) -> str`\n","\n","- Generates a response for queries that are not search queries.\n","- Handles greetings and non-search queries.\n","\n","## `requires_context(self, query:str) -> bool`\n","\n","- Checks if the input query requires context from previous queries.\n","- Identifies keywords indicating a need for context.\n"],"metadata":{"id":"5s2JSd-KJauZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Hq-jmUuR_ds"},"outputs":[],"source":["class ChatbotWrapper:\n","    def __init__(self):\n","        self.model = None\n","        self.tokenizer = None\n","        if torch.cuda.is_available():\n","            self.device = torch.device(\"cuda:0\")  # Use GPU if CUDA is available\n","\n","        else:\n","            self.device = torch.device(\"cpu\")\n","        self.repository_path = '/content/files_for_chatbot'\n","        self.EMBED_MODEL = 'sentence-transformers/all-mpnet-base-v2'\n","        self.DIM = 768\n","        self.collection = None\n","        self.collection_name = \"chat_demo\"\n","        self.MILVUS_URI = 'http://localhost:19530'\n","        [self.MILVUS_HOST, self.MILVUS_PORT] = self.MILVUS_URI.split('://')[1].split(':')\n","        self.llama_tokenizer=None\n","        self.llama_model=None\n","        self.result=\"\"\n","        self.embeddings=dict()\n","        self.history=list()\n","    def initialize_model(self):\n","        # Load the pre-trained model and tokenizer\n","        model_name = self.EMBED_MODEL\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n","\n","    def load_json_data(self):\n","        json_files = []\n","        for root, directories, files in os.walk(self.repository_path):\n","            for file in files:\n","                if file.endswith('.json'):\n","                    file_path = os.path.join(root, file)\n","                    json_files.append(file_path)\n","        return json_files\n","    def preprocess(self,json_data):\n","        # Convert each JSON entry into a single text string\n","        text_data = \"\"\n","        for key, value in json_data.items():\n","            if isinstance(value, list):\n","                # Convert list to string\n","                if key==\"Years\":\n","                    for i in range(len(value)):\n","                        if value[i]== \"0\" or value[i]==0:\n","                            value[i]=\"UKG\"\n","                        if value[i]==\"-1\" or value[i]==-1:\n","                            value[i]=\"LKG\"\n","                        if value[i]==\"-2\" or value[i]==-2:\n","                            value[i]=\"Nursery\"\n","                value_str = ', '.join(map(str, value))\n","                text_data += f\"{key}: {value_str}~\"\n","            else:\n","                if value==\"0\" or value==0 or value==-1 or value==\"-1\":\n","                    value=\"Not Available\"\n","\n","                text_data += f\"{key}: {value}~\"\n","\n","        return text_data\n","    def embedding(self,text_data):\n","\n","        inputs = self.tokenizer(text_data, return_tensors='pt', padding=True, truncation=True)\n","        if self.device.type == 'cuda':\n","            inputs = {key: tensor.cuda() for key, tensor in inputs.items()}  # Move tensors to CUDA\n","        if self.device.type == 'xla':\n","            inputs = {key: tensor.to(self.device) for key, tensor in inputs.items()}# Move tensors to TPU\n","        with torch.no_grad():\n","            # Forward pass through the model\n","            outputs = self.model(**inputs)\n","\n","\n","        embeddings = outputs.last_hidden_state.mean(dim=1)  # Assuming you want to use mean pooling\n","\n","        # Normalize the embeddings if needed\n","        embeddings=embeddings[0]\n","\n","        normalized_embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=-1)\n","\n","        return normalized_embeddings\n","\n","    def preprocess_and_embed(self, json_files):\n","        # Your preprocessing and embedding logic here\n","        embedded_data={}\n","        # Define the embedding pipeline\n","        model_name=self.EMBED_MODEL\n","        #]model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModel.from_pretrained(model_name)\n","\n","        # Define the embedding pipeline\n","        self.model=self.model.to(self.device)\n","\n","        # Load and process JSON data\n","        sumo=0\n","        sett=set()\n","        embedded_list=dict()\n","\n","        print(json_files)\n","\n","        for i in json_files:\n","\n","            with open(i, 'r') as file:\n","                json_data = json.load(file)\n","\n","            # Preprocess and embed each JSON entry\n","            sumo+=len(json_data)\n","\n","\n","            for entry in json_data:\n","                curr_dict={}\n","                text_data = self.preprocess(entry)\n","                if text_data not in sett:\n","                    sett.add(text_data)\n","                    embedded_list[text_data]=self.embedding(text_data)\n","        return embedded_list\n","\n","\n","    def create_collection(self,collection_name):\n","        connections.connect(host=self.MILVUS_HOST, port=self.MILVUS_PORT)\n","\n","        has_collection = utility.has_collection(collection_name)\n","\n","        if has_collection:\n","            utility.drop_collection(collection_name)\n","        # Create collection\n","        fields = [\n","            FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n","            FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=self.DIM),\n","            FieldSchema(name='text', dtype=DataType.VARCHAR, max_length=1000)\n","        ]\n","        schema = CollectionSchema(\n","            fields=fields,\n","            description=\"Towhee demo\",\n","            enable_dynamic_field=True\n","        )\n","        collection = Collection(name=collection_name, schema=schema)\n","\n","        # Change index here if you want to accelerate search\n","        index_params = {\n","            'metric_type': 'IP',\n","            'index_type': 'IVF_FLAT',\n","            'params': {'nlist': 4096,'nprobe':256}\n","        }\n","        collection.create_index(\n","            field_name='embedding',\n","            index_params=index_params\n","        )\n","\n","        return collection\n","\n","    def prepare_data(self):\n","        data = []\n","        listed_data=[]\n","        embedded_data=self.embeddings\n","        for text_data, embedding in embedded_data.items():\n","            vector_id = str(uuid.uuid4())\n","\n","            entity = {\n","                'embedding': embedding,\n","                'text': text_data\n","            }\n","\n","            data.append(entity)\n","        return data\n","    def insert_data(self,collection_name):\n","        self.collection = self.create_collection(collection_name)\n","\n","        # Prepare data\n","        data_to_insert = self.prepare_data()\n","        # Insert data into the collection\n","        self.collection.insert(data_to_insert)\n","\n","    def load_llama(self):\n","        bnb_config = BitsAndBytesConfig(load_in_8bit=True,\n","                                    bnb_8bit_use_double_quant=True,\n","                                    bnb_8bit_quant_type=\"nf4\",\n","                                    bnb_8bit_compute_dtype=torch.bfloat16)\n","        self.llama_tokenizer = AutoTokenizer.from_pretrained(\"4bit/Llama-2-7b-chat-hf\")\n","        self.llama_model = AutoModelForCausalLM.from_pretrained(\"4bit/Llama-2-7b-chat-hf\",quantization_config=bnb_config)\n","        self.llama_model.temperature=1.0\n","        self.llama_model.top_p=0.95\n","\n","        # Load JSON data asynchronously\n","\n","\n","\n","\n","    def run(self,collection_name=None):\n","        if collection_name is None:\n","            collection_name = self.collection_name\n","\n","        # Initialize model asynchronously\n","        self.initialize_model()\n","        json_files = self.load_json_data()\n","\n","        # Preprocess and embed data asynchronously\n","        self.embeddings =self.preprocess_and_embed(json_files)\n","\n","        # Insert data asynchronously\n","        self.insert_data(collection_name)\n","\n","        # Load llama model asynchronously\n","        self.load_llama()\n","\n","\n","    def search_milvus(self,query,collection_name):\n","        embedded_vec=self.embedding(query).cpu().numpy()\n","        collection=Collection(name=collection_name)\n","        collection.load()\n","        res=collection.search(\n","            data=[embedded_vec],\n","            anns_field=\"embedding\",\n","            param={\n","            'metric_type': 'IP',\n","            'params': {'nlist': 4096}\n","                    },\n","            limit=3,\n","\n","            output_fields=[\"text\"]   )\n","\n","        text_li=list()\n","        id_li=list()\n","        dist_li=list()\n","        for i, hits in enumerate(res):\n","\n","            for hit in hits:\n","                id_li.append(hit.entity.id)\n","                dist_li.append(hit.entity.distance)\n","                text_li.append(hit.entity.get(\"text\"))\n","        data=dict()\n","        data[\"id\"]=id_li\n","        data[\"dist\"]=dist_li\n","        data[\"text\"]=text_li\n","        return data\n","\n","\n","\n","    def generate_chat_response(self,query:str,data:dict)->str:\n","        search_results = data[\"text\"]\n","\n","\n","        input_prompt=f\"\"\"<<SYS>>You are a chatbot assistant. Generate the concise answer for the user based on query and search results and nothing else in response field.\n","        <</SYS>>\n","        [INST]\n","        User Query: {query}\n","        Search results: {search_results}\n","        [/INST]\n","        \"\"\"\n","\n","        inputs =self.llama_tokenizer([input_prompt], return_tensors = \"pt\").to(\"cuda\")\n","        outputs = self.llama_model.generate(**inputs, max_new_tokens=1000,pad_token_id=self.llama_model.config.eos_token_id)\n","        result_prompt = self.llama_tokenizer.decode(outputs[0])\n","        result_prompt=result_prompt.replace(input_prompt,\"\")\n","        di=dict()\n","        di[\"query\"]=query\n","        if len(self.history)==3:\n","            self.history.pop(0)\n","        self.history.append(di)\n","        return result_prompt.replace(input_prompt,\"\").replace(\"<s>\",\"\").replace(\"</s>\",\"\")\n","    def search(self,query:str,collection_name=None):\n","        if collection_name==None:\n","            collection_name=self.collection_name\n","        if self.is_query(query):\n","            print(\"IS a query\")\n","            if self.requires_context(query):\n","                print(\"Requires context\")\n","                new_query=self.generate_query(query)\n","\n","                data=self.search_milvus(new_query,collection_name)\n","                response=self.generate_chat_response(new_query,data)\n","                print(new_query)\n","                return response\n","            else:\n","                data=self.search_milvus(query,collection_name)\n","                response=self.generate_chat_response(query,data)\n","                return response\n","\n","\n","        response=self.not_query_response(query)\n","        return response\n","\n","    def generate_query(self,query:str)->str:\n","        input_prompt=f\"\"\"\n","        <<SYS>>\n","            Generate a question similar to previous queries, and given user query, make it concise and do not generate an answer to the query.\n","        <</SYS>>\n","        [INST]\n","        History: {self.history}\n","        Query: {query}\n","        [/INST]\n","        \"\"\"\n","        EOS_TOKEN = self.llama_tokenizer.eos_token\n","        inputs = self.llama_tokenizer(input_prompt, return_tensors=\"pt\",padding=True).to(\"cuda\")\n","        outputs = self.llama_model.generate(**inputs, max_new_tokens=1000)\n","        result_prompt = self.llama_tokenizer.batch_decode(outputs)[0]\n","        return result_prompt.replace(input_prompt,\"\").replace(\"<s>\",\"\").replace(\"</s>\",\"\")\n","    def is_query(self,query:str)->bool:\n","        query=query.lower()\n","        my_list=[\"school\", \"schools\", \"facilities\", \"amenities\", \"sports\", \"faculty\", \"fees\",\"institute\",\"organisation\",\"org\",\"inst\",\"scl\",\"schol\"]\n","        if any(item in query for item in my_list):\n","            return True\n","        return False\n","    def not_query_response(self,query:str):\n","        print(\"Not a query\")\n","        query=query.lower()\n","        input_prompt=f\"\"\"\n","            <<SYS>>\n","                You are a greeting checker and check if the sentence is explicitly greeting or not.\n","                Explain if it is a greeting or not.If it is NOT a greeting, i mean not a greeting then and then only please must include number 10393 in response.\n","            <</SYS>>\n","            [INST]\n","            Sentence: {query}\n","            [/INST]\n","            \"\"\"\n","        EOS_TOKEN = self.llama_tokenizer.eos_token\n","        inputs = self.llama_tokenizer(input_prompt, return_tensors=\"pt\",padding=True).to(\"cuda\")\n","        outputs = self.llama_model.generate(**inputs, max_new_tokens=1000)\n","        result_prompt = self.llama_tokenizer.batch_decode(outputs)[0]\n","        result_prompt=result_prompt.replace(input_prompt,\"\")\n","        if \"10393\" in result_prompt:\n","            return \"Please enter proper query\",result_prompt\n","        greeting_prompt=f\"\"\"\n","            <<SYS>>\n","                Please respond to the greeting appropriately and concisely\n","            <</SYS>>\n","            [INST]\n","            Greeting: {query}\n","            [/INST]\n","            \"\"\"\n","        inputs = self.llama_tokenizer(greeting_prompt, return_tensors=\"pt\",padding=True).to(\"cuda\")\n","        outputs = self.llama_model.generate(**inputs, max_new_tokens=1000)\n","        result=self.llama_tokenizer.batch_decode(outputs)[0]\n","        result=result.replace(greeting_prompt,\"\")\n","        return result\n","    def requires_context(self,query:str)->bool:\n","        query=query.lower()\n","        context_keywords = [\"previous\", \"last\", \"before\", \"history\", \"context\",\"same\"]\n","        if any(item in query for item in context_keywords):\n","            return True\n","        return False"]},{"cell_type":"markdown","source":["## Initialize Chatbot\n","\n","Instantiate the `ChatbotWrapper` class to create a chatbot instance. This instance will be used to handle interactions and queries within the chatbot environment.\n","\n"],"metadata":{"id":"c5sHhRSpJQHy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGTk4jzRroN4"},"outputs":[],"source":["chatbot=ChatbotWrapper()"]},{"cell_type":"markdown","source":["## Clean Up Existing Milvus Server\n","\n","This cell ensures that any existing Milvus server instances are cleaned up before starting a new one.\n"],"metadata":{"id":"VpkuZwEhJ8Fq"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"ZSfhyt0e3QIX","executionInfo":{"status":"error","timestamp":1717033187514,"user_tz":-330,"elapsed":12,"user":{"displayName":"Suhail Azad","userId":"04231081887048465977"}},"colab":{"base_uri":"https://localhost:8080/","height":141},"outputId":"8d2ae10d-0709-448b-f30a-868abda0d387"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'default_server' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d74855308fba>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdefault_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'default_server' is not defined"]}],"source":["default_server.cleanup()"]},{"cell_type":"markdown","source":["## Start Milvus Server\n","\n","This cell starts a new instance of the Milvus server, which will be used to store and search embeddings generated by the chatbot."],"metadata":{"id":"AWjcE4y0J9en"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOp93q8sIqMi"},"outputs":[],"source":["default_server.start()"]},{"cell_type":"markdown","source":["## Run Chatbot\n","\n","This cell executes the necessary steps to set up and run the chatbot. It initializes the model, preprocesses and embeds data, inserts data into the Milvus collection, and loads the llama model for generating responses.\n"],"metadata":{"id":"AkLz94QXKDoL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOUSIC43IcXg"},"outputs":[],"source":["chatbot.run()"]},{"cell_type":"markdown","source":["## Manual Query Search (Functionality Check)\n","\n","This cell allows for manual testing of the chatbot's search functionality before deploying it into Gradio. Users can input queries directly, and the chatbot will provide responses based on its current configuration and data. This step is essential for verifying the functionality and accuracy of the chatbot's search capabilities before integration with a user interface like Gradio.\n"],"metadata":{"id":"YtITfhJoKF0A"}},{"cell_type":"code","source":["query=input(\"Enter your query:\")\n","print(chatbot.search(query))"],"metadata":{"id":"MYHGAHl940QR","executionInfo":{"status":"ok","timestamp":1716725087954,"user_tz":-330,"elapsed":192030,"user":{"displayName":"Suhail Azad","userId":"04231081887048465977"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0e76e961-d387-47b9-a3f7-8ad595d4a9ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter your query:ICSE schools in secunderabad\n","IS a query\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":[" 1. The Secunderabad Public School\n","            - Name: The Secunderabad Public School\n","            - Category: Public Schools\n","            - Location: D.No. 2-12-70, West Marredpally, Gandhi Nagar, Nehru Nagar Colony, West Marredpally, Secuderabad, Telangana 500026, India\n","            - Faculty: Not Available\n","            - Sports: Not Available\n","            - Amenities: Transport, Medical Facility, Laboratory, Computers Facility, Library\n","            - Board: CBSE\n","            - Years: LKG, UKG, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n","            - Fee: -1\n","            - Since: 1984\n","            - Strength: Not Available\n","        2. Indian School Of Excellence\n","            - Name: Indian School Of Excellence\n","            - Category: Special Education Schools, Public Schools\n","            - Location: #8-1-40291, Gulshan Colony, Opposite Masjid Abu Bakar, Tolichowki, Hyderabad-8, Telangana, India\n","            - Faculty: Basketball, Carroms, Chess, Cricket, Football, Karate, Throwball, Volleyball\n","            - Amenities: Transport, Medical Facility, Laboratory, Smart Classrooms, Computers Facility, Library\n","            - Board: ICSE\n","            - Years: LKG, UKG, 1, 2, 3, 4, 5, 6, 7\n","            - Fee: 10000, 15000\n","            - Since: 2004\n","            - Strength: Not Available\n","        3. Shantiniketan International School\n","            - Name: Shantiniketan International School\n","            - Category: Other\n","            - Location: Sainikpuri Rd, G K Colony, Saptagiri Colony, Sainikpuri, Secunderabad, Telangana 500056, India\n","            - Faculty: Not Available\n","            - Sports: Not Available\n","            - Amenities: Transport\n","            - Board: International Baccalaureate, IGCSE, CBSE\n","            - Years: UKG\n","            - Fee: 111000\n","            - Since: Not Available\n","            - Strength: Not Available\n"]}]},{"cell_type":"markdown","source":["## Chatbot Response Function\n","\n","This cell defines a function `respond(query)` that interacts with the chatbot's search functionality. The function takes a query as input, searches for a response using the chatbot, and appends the query-response pair to the conversation history stored in the `convo` list. This function is intended to be used within a Gradio interface to facilitate conversations with the chatbot.\n"],"metadata":{"id":"xHgWj8asKnDb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DRJn7O9WDq11"},"outputs":[],"source":["convo=[]\n","def respond(query):\n","    global convo\n","    res=chatbot.search(query)\n","    li=[query,res]\n","    convo.append(li)\n","    return convo\n"]},{"cell_type":"markdown","source":["## Gradio Interface Setup\n","\n","This cell sets up a Gradio interface for interacting with the chatbot. The interface consists of a chat window where users can view the conversation history and input their questions. Upon clicking the \"Send Message\" button, the `respond()` function is called to process the user's query and generate a response from the chatbot. The conversation history is updated in real-time as the user interacts with the chatbot.\n"],"metadata":{"id":"qhpnsZbRK-N0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ADN3H7p3CGcX"},"outputs":[],"source":["import gradio as gr\n","\n","with gr.Blocks() as demo:\n","\n","    with gr.Row():\n","        with gr.Column(scale=2):\n","            gr.Markdown('''## Chat''')\n","            conversation = gr.Chatbot(label='conversation')\n","            question = gr.Textbox(label='question', value=None)\n","            send_btn = gr.Button('Send Message')\n","            send_btn.click(\n","                fn=respond,\n","                inputs=[\n","                   question\n","                ],\n","                outputs=conversation,\n","            )"]},{"cell_type":"markdown","source":["## Launch Gradio Interface\n","\n","This cell launches the Gradio interface for interacting with the chatbot in debug mode. The debug mode allows for real-time testing and debugging of the interface. Users can input queries, view responses, and observe the conversation history within the interface. Any errors or issues encountered during interaction can be identified and addressed effectively in this mode.\n"],"metadata":{"id":"ogq7P4ARLpCm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2ycaK-UCqU7","colab":{"base_uri":"https://localhost:8080/","height":975},"executionInfo":{"status":"ok","timestamp":1716725847512,"user_tz":-330,"elapsed":723386,"user":{"displayName":"Suhail Azad","userId":"04231081887048465977"}},"outputId":"50bb2f14-5ef0-4959-8daa-b659a65b854e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Running on public URL: https://4b84dcbf975f4cfe98.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://4b84dcbf975f4cfe98.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["IS a query\n","Requires context\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["  Great! Here's a concise question similar to the previous query:\n","\n","\"Which CBSE schools are located in the same area as the ICSE schools in Secunderabad?\"\n","Not a query\n","Not a query\n","IS a query\n","IS a query\n","Requires context\n","  Sure, here's a concise question similar to the previous query:\n","\n","\"Which schools in Secunderabad offer the same requirements as the ICSE schools in the area?\"\n","Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://4b84dcbf975f4cfe98.gradio.live\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":14}],"source":["demo.launch(debug=True)"]},{"cell_type":"markdown","source":["## Close Gradio Interface\n","\n","This cell closes the Gradio interface, terminating the interaction with the chatbot. It's typically used when you're done testing or debugging the interface and want to shut it down gracefully.\n"],"metadata":{"id":"Hj5LYvzULvmd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zSzBkAy6mBWV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716725848190,"user_tz":-330,"elapsed":2,"user":{"displayName":"Suhail Azad","userId":"04231081887048465977"}},"outputId":"a1f14d3c-f91d-477a-ef6a-56d3a41f60d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Closing server running on port: 7860\n"]}],"source":["demo.close()"]},{"cell_type":"markdown","source":["## Stop Milvus Server\n","\n","This cell stops the Milvus server, terminating the storage and search functionality associated with the chatbot's embeddings. It's essential to stop the server after completing the chatbot interactions to release resources and ensure proper cleanup.\n"],"metadata":{"id":"RQJXDlD6L4aq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyv1JCrrl7eK"},"outputs":[],"source":["default_server.stop()"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}